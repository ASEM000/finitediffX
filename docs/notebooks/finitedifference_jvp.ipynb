{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¢ Finite difference `jvp` rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`define_fdjvp` combines `custom_jvp` and `fgrad` to define custom finite difference rules,when used with `pure_callback` it can to make non-tracable code works within `jax` machinary.\n",
    "\n",
    "_This example is based on the comment from `jax` proposed [`JEP`](https://github.com/google/jax/issues/15425)_\n",
    "\n",
    "For example this code will fail to work with `jax` transformations, becasue it uses `numpy` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numpy.ndarray conversion method __array__() was called on traced array with shape float32[].\n",
      "See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError\n",
      "1.9999794\n",
      "2.0000048\n"
     ]
    }
   ],
   "source": [
    "import functools as ft\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as onp\n",
    "import finitediffx as fdx\n",
    "import functools as ft\n",
    "\n",
    "\n",
    "def numpy_func(x: onp.ndarray) -> onp.ndarray:\n",
    "    return onp.power(x, 2)\n",
    "\n",
    "\n",
    "try:\n",
    "    jax.grad(numpy_func)(2.0)\n",
    "except jax.errors.TracerArrayConversionError as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "def wrap_pure_callback(func):\n",
    "    @ft.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        args = [jnp.asarray(arg) for arg in args]\n",
    "        func_ = lambda *a, **k: func(*a, **k).astype(a[0].dtype)\n",
    "        dtype_ = jax.ShapeDtypeStruct(\n",
    "            jnp.broadcast_shapes(*[ai.shape for ai in args]),\n",
    "            args[0].dtype,\n",
    "        )\n",
    "        return jax.pure_callback(func_, dtype_, *args, **kwargs, vectorized=True)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@jax.jit  # -> can compile\n",
    "@jax.grad  # -> can take gradient\n",
    "@ft.partial(\n",
    "    fdx.define_fdjvp,\n",
    "    # automatically generate offsets\n",
    "    offsets=fdx.Offset(accuracy=4),\n",
    "    # manually set step size\n",
    "    step_size=1e-3,\n",
    ")\n",
    "@wrap_pure_callback\n",
    "def numpy_func(x: onp.ndarray) -> onp.ndarray:\n",
    "    return onp.power(x, 2)\n",
    "\n",
    "\n",
    "print(numpy_func(1.0))\n",
    "# 1.9999794\n",
    "\n",
    "\n",
    "@jax.jit  # -> can compile\n",
    "@jax.grad  # -> can take gradient\n",
    "@ft.partial(\n",
    "    fdx.define_fdjvp,\n",
    "    # provide the desired evaluation points for the finite difference stencil\n",
    "    # in this case its centered finite difference (f(x-1) - f(x+1))/(2*step_size)\n",
    "    offsets=jnp.array([1, -1]),\n",
    "    # manually set step size\n",
    "    step_size=1e-3,\n",
    ")\n",
    "@wrap_pure_callback\n",
    "def numpy_func(x: onp.ndarray) -> onp.ndarray:\n",
    "    return onp.power(x, 2)\n",
    "\n",
    "\n",
    "print(numpy_func(1.0))\n",
    "# 2.0000048"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
